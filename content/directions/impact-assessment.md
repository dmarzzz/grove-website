---
title: Sovereign AI Assessment
numeral: III
descriptor: Benchmarking the stack. Ranking what runs.
---

A systematic audit of local and open-source AI infrastructure — inference engines, model architectures, fine-tuning toolchains. We develop standardized benchmarks and evals to rank open-source language models against the specific workflows used at the school: transcription, knowledge linking, document analysis, local agent orchestration. The goal is a practical map of leverage: which layers are mature, which are fragile, and where an hour of work does the most to make local-first AI real.

→ Standardized benchmarks and evals for open-source LMs against school workflows
→ Community meetups on local LLMs and personal AI assistants (Clowdbot++)
